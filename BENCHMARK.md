# ğŸ§ª SystÃ¨me de Benchmark IA

## Vue d'ensemble

Ce systÃ¨me de benchmark teste **vitesse ET exactitude** du bot IA sur 20 questions reprÃ©sentatives.

### MÃ©triques mesurÃ©es :
- âœ… **Exactitude** : % de tests rÃ©ussis, score moyen (0-100%)
- â±ï¸ **Vitesse** : Temps de rÃ©ponse moyen et mÃ©dian
- ğŸ¯ **PrÃ©cision** : Validation automatique des rÃ©ponses (mots-clÃ©s, outils appelÃ©s, etc.)

## ğŸ“‹ Les 20 questions de test

Les questions couvrent tous les cas d'usage critiques :

### Factures (8 tests)
- Factures d'un fournisseur spÃ©cifique
- Toutes les factures (avec/sans filtre)
- Factures par pÃ©riode
- Factures impayÃ©es
- Questions naturelles ("Montre-moi ce qu'on a payÃ©...")

### Salaires/EmployÃ©s (5 tests)
- Salaires d'un employÃ©
- Recherche avec nom partiel
- Top X employÃ©s
- Comparaisons
- PÃ©riodes multi-mois

### Fournisseurs/DÃ©penses (5 tests)
- Top X fournisseurs
- Analyse de dÃ©penses
- Factures par catÃ©gorie
- Comparaisons
- Questions naturelles

### Transactions/Analytics (2 tests)
- DerniÃ¨re transaction
- PrÃ©dictions

## ğŸš€ Utilisation

### 1. Benchmark AVANT optimisations

```bash
npm run benchmark:before
```

**RÃ©sultat** : Fichier `data/benchmarks/benchmark-before-{timestamp}.json`

### 2. Benchmark APRÃˆS optimisations

```bash
npm run benchmark:after
```

**RÃ©sultat** : Fichier `data/benchmarks/benchmark-after-{timestamp}.json`

### 3. Comparer les rÃ©sultats

```bash
npm run benchmark:compare benchmark-before-{timestamp}.json benchmark-after-{timestamp}.json
```

**Affiche** :
- Delta d'exactitude (%)
- Delta de vitesse (ms et %)
- Tests qui ont changÃ© de statut
- Verdict final

## ğŸ“Š CritÃ¨res de validation

Chaque question a des critÃ¨res spÃ©cifiques :

### Exemple : "Factures de Foster"

```typescript
{
  mustContain: ['Foster'],           // Mot-clÃ© obligatoire
  expectedTool: 'get_recent_invoices', // Outil attendu
  minDataPoints: 1                    // Minimum 1 facture retournÃ©e
}
```

### Scoring

- **Mots-clÃ©s prÃ©sents** : 30 points
- **Mots-clÃ©s interdits absents** : 20 points
- **Patterns regex matchÃ©s** : 20 points
- **Outil correct appelÃ©** : 20 points
- **DonnÃ©es suffisantes** : 10 points

**Seuil de rÃ©ussite** : 70%

## ğŸ“ Structure des fichiers

```
src/
â”œâ”€â”€ benchmark.ts                 # Point d'entrÃ©e principal
â””â”€â”€ benchmark/
    â”œâ”€â”€ test-questions.ts        # 20 questions avec critÃ¨res
    â”œâ”€â”€ validator.ts             # Logique de validation
    â”œâ”€â”€ runner.ts                # ExÃ©cution des tests
    â””â”€â”€ compare.ts               # Comparaison avant/aprÃ¨s

data/benchmarks/
â”œâ”€â”€ benchmark-before-*.json      # RÃ©sultats AVANT
â””â”€â”€ benchmark-after-*.json       # RÃ©sultats APRÃˆS
```

## ğŸ¯ Objectifs d'optimisation

### Cibles attendues :

| MÃ©trique | Avant | Cible AprÃ¨s | Gain |
|----------|-------|-------------|------|
| Temps moyen | 2500ms | 1000ms | **60% plus rapide** |
| Temps mÃ©dian | 2200ms | 900ms | **59% plus rapide** |
| Exactitude | 60-70% | 90-95% | **+25-35%** |
| Score moyen | 75% | 90% | **+15 points** |

## ğŸ’¡ InterprÃ©tation des rÃ©sultats

### Score < 70% (Ã©chec)
- RÃ©ponse incorrecte ou incomplÃ¨te
- Outil inappropriÃ© utilisÃ©
- DonnÃ©es manquantes

### Score 70-85% (partiel)
- RÃ©ponse globalement correcte
- Quelques mots-clÃ©s manquants
- Outil correct mais paramÃ¨tres sous-optimaux

### Score > 85% (succÃ¨s)
- RÃ©ponse prÃ©cise et complÃ¨te
- Bon outil avec bons paramÃ¨tres
- Toutes les donnÃ©es prÃ©sentes

## ğŸ”§ Ajouter de nouveaux tests

Modifier `src/benchmark/test-questions.ts` :

```typescript
{
  id: 'NEW-001',
  category: 'Votre catÃ©gorie',
  question: 'Votre question',
  expectedBehavior: 'Description du comportement attendu',
  validationCriteria: {
    mustContain: ['mot1', 'mot2'],
    expectedTool: 'nom_outil',
    minDataPoints: 5
  }
}
```

## ğŸ“ˆ Workflow recommandÃ©

1. **Baseline** : `npm run benchmark:before`
2. **Optimiser** le code (supprimer classification, hints, etc.)
3. **Tester** : `npm run benchmark:after`
4. **Comparer** : `npm run benchmark:compare before.json after.json`
5. **Valider** : Si score > 90% ET vitesse > +50%, merge !

## âš ï¸ Notes importantes

- Les tests font de **vrais appels API** (Billit, OpenRouter)
- Pause de 500ms entre chaque test (rate limiting)
- DurÃ©e totale : ~15-20 minutes pour 20 tests
- CoÃ»t estimÃ© : ~$0.05 par run (GPT-4o-mini)

## ğŸ› DÃ©pannage

**Erreur "Aucun provider IA disponible"**
â†’ VÃ©rifier `.env` : `OPENROUTER_API_KEY` ou `GROQ_API_KEY`

**Tests Ã©chouent tous**
â†’ VÃ©rifier les credentials Billit dans `.env`

**Timeout**
â†’ Augmenter le timeout dans `runner.ts` (ligne ~150)

---

**DerniÃ¨re mise Ã  jour** : 22 janvier 2026
